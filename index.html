<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
    <title>RoboTwin 2.0</title>
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="manifest" href="/site.webmanifest">

    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://robotwin-platform/.github.io/"/>
    <meta property="og:title" content="Robot Simulation Platform"/>
    <meta property="og:description" content="RoboTwin 2.0"/>

    <!-- twitter card -->
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title" content="Robot Simulation Platform"/>
    <meta name="twitter:description"
          content="RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation"/>
    <meta name="twitter:creator" content="@Tianxing Chen"/>


    <!-- extra metadata — unknown support -->
    <meta property="og:type" content="article"/>
    <meta property="article:section" content="Research"/>
    <meta property="article:tag" content="Robotics"/>
    <meta property="article:tag" content="Machine Learning"/>

</head>
<body>

<div class="title_left">
    <h1>
        <span style="color:#65a487; font-size: larger;">Robo</span><span style="color:#68349a; font-size: larger;">Twin</span>
        <span>2.0: A Scalable Data Generator and Benchmark with <br>Strong Domain Randomization for Robust Bimanual Robotic Manipulation</span>
    </h1>      
    <div class="author-container">
        <div class="author-name">
            <strong><a href="https://tianxingchen.github.io/" target="_blank">Tianxing Chen</a></strong><sup>2,16*</sup>,
        </div>
        <div class="author-name">
            <strong>Zanxin Chen</strong><sup>3,5*</sup>,
        </div>
        <div class="author-name">
            <strong>Baijun Chen</strong><sup>15*</sup>,
        </div>
        <div class="author-name">
            <strong>Zijian Cai</strong><sup>3,5*</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://10-oasis-01.github.io" target="_blank">Yibin Liu</a></strong><sup>13*</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong><a href="https://kolakivy.github.io/" target="_blank">Qiwei Liang</a></strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Zixuan Li</strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Xianliang Lin</strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://geyiheng.github.io" target="_blank">Yiheng Ge</a></strong><sup>1</sup>,
        </div>
        <div class="author-name">
            <strong>Zhenyu Gu</strong><sup>7,8</sup>,
        </div>
        <div class="author-name">
            <strong>Weiliang Deng</strong><sup>3,11</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong>Yubin Guo</strong><sup>7,9</sup>,
        </div>
        <div class="author-name">
            <strong>Tian Nian</strong><sup>3,5</sup>,
        </div>
        <div class="author-name">
            <strong>Xuanbing Xie</strong><sup>12</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://www.linkedin.com/in/yusen-qin-5b23345b/" target="_blank">Qiangyu Chen</a></strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Kailun Su</strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Tianling Xu</strong><sup>10</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong><a href="http://luoping.me/" target="_blank">Guodong Liu</a></strong><sup>6,7</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://aaron617.github.io/" target="_blank">Mengkang Hu</a></strong><sup>2</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://c7w.tech/about" target="_blank">Huan-ang Gao</a></strong><sup>6,16</sup>,
        </div>
        <div class="author-name">
            <strong>Kaixuan Wang</strong><sup>2,16</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://liang-zx.github.io/" target="_blank">Zhixuan Liang</a></strong><sup>2,3</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong><a href="https://www.linkedin.com/in/yusen-qin-5b23345b/" target="_blank">Yusen Qin</a></strong><sup>4,6</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://scholar.google.com/citations?user=yDEavdMAAAAJ&hl=zh-CN" target="_blank">Xiaokang Yang</a></strong><sup>1</sup>,
        </div>
        <div class="author-name">
            <strong><a href="http://luoping.me/" target="_blank">Ping Luo</a></strong><sup>2,14✉</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://yaomarkmu.github.io/" target="_blank">Yao Mu</a></strong><sup>1,3✉</sup>
        </div>
    </div>
    
    <style>
        .author-container {
            color: black;
            font-family: "Source Serif 4", serif;
            font-weight: normal;
            font-size: 1vw;
            padding-top: 1.0vw;
            justify-content: center; 
            display: flex;
            flex-wrap: wrap; 
            align-items: center; 
            text-align: center; 
            white-space: normal; 
        }
        .author-name {
            margin: 0 0.0em;
            white-space: normal; 
        }
        .flex-break {
            width: 100%; 
        }
    </style>
        
    <div class="affiliation" style="white-space: nowrap;">
        <p><sup>1</sup>SJTU ScaleLab<sup>†</sup>,&nbsp;<sup>2</sup>HKU MMLab<sup>†</sup>,&nbsp; 
           <sup>3</sup>Shanghai AI Lab,&nbsp; <sup>4</sup>D-Robotics,&nbsp; 
           <sup>5</sup>SZU,&nbsp; <sup>6</sup>THU,&nbsp;
           <sup>7</sup>TeleAI,&nbsp; <br> <sup>8</sup>FDU,&nbsp;
           <sup>9</sup>USTC,&nbsp; <sup>10</sup>SUSTech,&nbsp;
           <sup>11</sup>SYSU,&nbsp; <sup>12</sup>CSU,&nbsp;
           <sup>13</sup>NEU,&nbsp; <sup>14</sup>HKU-Shanghai ICRC,&nbsp; <sup>15</sup>NJU,&nbsp;
           <sup>16</sup>Lumina EAI&nbsp; <br> <sup>*</sup>Equal contribution&nbsp;
           <sup>✉</sup>Corresponding authors&nbsp;<sup>†</sup>Equally leading organizations&nbsp;
        </p>
    </div>
    <br>
    <img src="assets/images/institute-logo.png" alt="描述信息" style="display: block; margin: auto; width: 50%;">
    <br><br>
    <div class="button-container">
         <a href="https://arxiv.org/abs/2506.18088" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
        <a href="https://arxiv.org/pdf/2506.18088" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
        <a href="https://github.com/robotwin-Platform/RoboTwin" target="_blank" class="button"><i class="fa fa-github"></i>&emsp14;Code</a>
        <a href="https://robotwin-platform.github.io/doc/" target="_blank" class="button_large"><i class="fa-regular fa-file-lines"></i>&emsp14;Document</a>
    </div>
</div>
           
    
<div id="abstract">
    <h1>Abstract</h1>
    <p>
        Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical evaluation shows a 10.9% gain in code generation success rate and improved generalization to novel real-world conditions. A vision-language-action (VLA) model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained exclusively on our synthetic data attain a 228% relative gain, demonstrating strong generalization without real-world supervision. We release the data generator, benchmark, pre-collected dataset, and code to support scalable research in robust bimanual manipulation.
    </p>
    <br>
    <!-- <div class="publication-video">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/6H2MkMetmFk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div> -->
    <div style="display: flex; justify-content: center; margin-top: 20px;">
        <video 
          src="https://robotwin-platform.github.io/RoboTwin2.0_video.mp4" 
          controls 
          autoplay 
          muted 
          loop 
          style="width: 100%; height: auto;"
        ></video>
      </div>
</div>

<hr class="rounded">



<div id="overview">
    <h1>Previous Works</h1>
    <p>
        <b>[RoboTwin 1.0, CVPR 2025 Highlight]</b> <a href="https://arxiv.org/abs/2504.13059">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</a><br>
        <b>[CVPR 2025 Challenge@MEIS Workshop]</b> <a href="https://arxiv.org/abs/2506.23351">Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop</a><br>
        <b>[RoboTwin early version, ECCV 2024 MAAS Workshop Best Paper]</b> <a href="https://arxiv.org/abs/2409.02920">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</a><br>
        <b>[第十九届挑战杯官方赛题]</b> <a href="https://2025.tiaozhanbei.net/media/ckeditor_uploads/49/2025/05/14/4.%E3%80%90%E9%A2%98%E7%9B%AE%E5%9B%9B%E3%80%91%E7%AB%AF%E4%BE%A7%E5%8F%AF%E9%83%A8%E7%BD%B2%E7%9A%84%E5%8F%8C%E8%87%82%E6%93%8D%E4%BD%9C%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1.pdf">赛题链接</a>
    </p>
    <h1>Overview of RoboTwin 2.0</h1>
    <img src="assets/images/teaser.png" alt="描述信息" style="display: block; margin: auto; width: 70%;">
    <p>
        RoboTwin 2.0 is a scalable framework for data generation and benchmarking in bimanual robotic manipulation. It integrates an expert data generation pipeline and a 50-task benchmark built on the RoboTwin Object Dataset (731 objects, 147 categories). A multimodal language model agent enables automatic task program synthesis, while flexible dual-arm configurations facilitate scalable and diverse data collection. Policies trained on RoboTwin 2.0 data demonstrate improved robustness and generalization to unseen environments.
    </p>

    <br>
    <hr class="rounded">


    <h1>RoboTwin Object Dataset</h1>
    <img src="assets/images/robotwin-od.png" alt="RoboTwin-OD" style="display: block; margin: auto; width: 70%;">

    <p>
        To enhance both manipulation capability and visual understanding, we construct a large-scale object dataset with rich semantic annotations, called <b>RoboTwin-OD</b>, covering 147 categories and 731 diverse objects. Specifically, this includes 534 instances across 111 categories with custom-generated and optimized meshes, 153 objects from 27 categories in Objaverse, and 44 articulated object instances from 9 categories in SAPIEN PartNet-Mobility. Objects from all sources, including Objaverse, are used for cluttered scene construction, with Objaverse specifically serving to further increase the visual and semantic diversity of distractor objects. Additionally, we develop a comprehensive surface and background texture library using generative AI and human-in-the-loop verification to ensure both diversity and realism. The dataset is available at <a href="https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/objects">https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/objects</a>.
    </p>

    <hr class="rounded">


    <h1>50 RoboTwin 2.0 Bimanual Tasks</h1>
    <img src="assets/images/50-tasks.gif" alt="描述信息" style="display: block; margin: auto; width: 70%;">

    <p>
        Building on our automated task generation framework, embodiment-adaptive behavior synthesis, and the large-scale object asset library RoboTwin-OD, we construct a suite of over 50 dual-arm collaborative manipulation tasks. In addition, we support data collection and evaluation across 5 distinct robot platforms, enabling comprehensive benchmarking of manipulation policies. The complete task set is available at <a href="http://robotwin-platform.github.io/doc/tasks/">http://robotwin-platform.github.io/doc/tasks/</a>.
    </p>

    <br>
    <hr class="rounded">
    
    <h1>Multi-Embodiments Support</h1>
    <img src="assets/images/cross-embodiment.png" alt="描述信息" style="display: block; margin: auto; width: 40%;">


    <br>
    <hr class="rounded">
    
    <h1>Domain Randomization</h1>
    <img src="assets/images/randomization.png" alt="RoboTwin-OD" style="display: block; margin: auto; width: 70%;">

    <p>
        To improve policy robustness to real-world environmental variability, we apply domain randomization across five key dimensions: (1) cluttered placement of task-irrelevant objects, (2) background textures, (3) lighting conditions, (4) tabletop heights, and (5) diverse language instructions. This systematic diversification enriches the training data distribution and significantly improves generalization to unseen scenarios.
    </p>
    
    <br>
    <hr class="rounded">

    <h1>Conclusions</h1>

    <p>
        This paper presented RoboTwin 2.0, a scalable simulation framework for generating diverse, high-fidelity expert data to support robust bimanual manipulation. Our system integrates MLLM-based task generation, embodiment-adaptive behavior synthesis, and comprehensive domain randomization to address key limitations in prior synthetic datasets.
        <br>
By leveraging an annotated object library and automating trajectory generation, RoboTwin 2.0 produces data with rich visual, linguistic, and physical diversity while minimizing manual engineering effort. Experiments demonstrate its effectiveness in improving policy robustness to cluttered environments, generalization to unseen tasks, and cross-embodiment manipulation.
<br>
These findings highlight the importance of scalable, automated generation of semantically rich, domain-randomized data for learning robust manipulation policies. RoboTwin 2.0 provides a foundation for unified benchmarks and scalable sim-to-real pipelines, with future work focusing on real-world deployment and multi-object task complexity.
    </p>

    <h1>BibTeX</h1>
    <p><b>[2.0 Version]</b> RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation</p>
    <p class="bibtex">@article{chen2025robotwin,<br>
        &nbsp;&nbsp;title={RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation},<br>
        &nbsp;&nbsp;author={Chen, Tianxing and Chen, Zanxin and Chen, Baijun and Cai, Zijian and Liu, Yibin and Liang, Qiwei and Li, Zixuan and Lin, Xianliang and Ge, Yiheng and Gu, Zhenyu and others},<br>
        &nbsp;&nbsp;journal={arXiv preprint arXiv:2506.18088},<br>
        &nbsp;&nbsp;year={2025}<br>
        }
    </p>

    <p>Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop</p>
    <p class="bibtex">@misc{chen2025benchmarkinggeneralizablebimanualmanipulation,<br>
        &nbsp;&nbsp;title={Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop},<br>
        &nbsp;&nbsp;author={Tianxing Chen and Kaixuan Wang and Zhaohui Yang and Yuhao Zhang and Zanxin Chen and Baijun Chen and Wanxi Dong and Ziyuan Liu and Dong Chen and Tianshuo Yang and Haibao Yu and Xiaokang Yang and Yusen Qin and Zhiqiang Xie and Yao Mu and Ping Luo and Tian Nian and Weiliang Deng and Yiheng Ge and Yibin Liu and Zixuan Li and Dehui Wang and Zhixuan Liang and Haohui Xie and Rijie Zeng and Yunfei Ge and Peiqing Cong and Guannan He and Zhaoming Han and Ruocheng Yin and Jingxiang Guo and Lunkai Lin and Tianling Xu and Hongzhe Bi and Xuewu Lin and Tianwei Lin and Shujie Luo and Keyu Li and Ziyan Zhao and Ke Fan and Heyang Xu and Bo Peng and Wenlong Gao and Dongjiang Li and Feng Jin and Hui Shen and Jinming Li and Chaowei Cui and Yuchen and Yaxin Peng and Lingdong Zeng and Wenlong Dong and Tengfei Li and Weijie Ke and Jun Chen and Erdemt Bao and Tian Lan and Tenglong Liu and Jin Yang and Huiping Zhuang and Baozhi Jia and Shuai Zhang and Zhengfeng Zou and Fangheng Guan and Tianyi Jia and Ke Zhou and Hongjiu Zhang and Yating Han and Cheng Fang and Yixian Zou and Chongyang Xu and Qinglun Zhang and Shen Cheng and Xiaohe Wang and Ping Tan and Haoqiang Fan and Shuaicheng Liu and Jiaheng Chen and Chuxuan Huang and Chengliang Lin and Kaijun Luo and Boyu Yue and Yi Liu and Jinyu Chen and Zichang Tan and Liming Deng and Shuo Xu and Zijian Cai and Shilong Yin and Hao Wang and Hongshan Liu and Tianyang Li and Long Shi and Ran Xu and Huilin Xu and Zhengquan Zhang and Congsheng Xu and Jinchang Yang and Feng Xu},<br>
        &nbsp;&nbsp;year={2025},<br>
        &nbsp;&nbsp;eprint={2506.23351},<br>
        &nbsp;&nbsp;archivePrefix={arXiv},<br>
        &nbsp;&nbsp;primaryClass={cs.RO},<br>
        &nbsp;&nbsp;url={https://arxiv.org/abs/2506.23351},<br>
        }
    </p>

    <p><b>[1.0 Version]</b> RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins. <b style="color: red">Accepted to CVPR 2025 (Highlight)</b></p>
    <br>
    <p class="bibtex">@InProceedings{Mu_2025_CVPR,<br>
        &nbsp;&nbsp;author    = {Mu, Yao and Chen, Tianxing and Chen, Zanxin and Peng, Shijia and Lan, Zhiqian and Gao, Zeyu and Liang, Zhixuan and Yu, Qiaojun and Zou, Yude and Xu, Mingkun and Lin, Lunkai and Xie, Zhiqiang and Ding, Mingyu and Luo, Ping},<br>
        &nbsp;&nbsp;title     = {RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins},<br>
        &nbsp;&nbsp;booktitle = {Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR)},<br>
        &nbsp;&nbsp;month     = {June},<br>
        &nbsp;&nbsp;year      = {2025},<br>
        &nbsp;&nbsp;pages     = {27649-27660}<br>
        }
    </p>
    <p><b>[Early Version]</b> RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version). Accepted to <b style="color: red">ECCV Workshop 2024 (Best Paper Award)</b></p>
    <br>
    <p class="bibtex">@article{mu2024robotwin,<br>
        &nbsp;&nbsp;title={RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)},<br>
        &nbsp;&nbsp;author={Mu, Yao and Chen, Tianxing and Peng, Shijia and Chen, Zanxin and Gao, Zeyu and Zou, Yude and Lin, Lunkai and Xie, Zhiqiang and Luo, Ping},<br>
        &nbsp;&nbsp;journal={arXiv preprint arXiv:2409.02920},<br>
        &nbsp;&nbsp;year={2024}<br>
        }
    </p>


    <br>    
</div>

</body>

</html>
