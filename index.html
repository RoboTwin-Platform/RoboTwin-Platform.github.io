<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <!-- <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
    <title>RoboTwin 2.0</title>
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="manifest" href="/site.webmanifest">

    <meta property="og:type" content="website"/>
    <meta property="og:url" content="https://robotwin-platform/.github.io/"/>
    <meta property="og:title" content="Robot Simulation Platform"/>
    <meta property="og:description" content="RoboTwin 2.0"/>

    <!-- twitter card -->
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title" content="Robot Simulation Platform"/>
    <meta name="twitter:description"
          content="RoboTwin 2.0: A Scalable Data Generator and Benchmark with <br>Strong Domain Randomization for Robust Bimanual Robotic Manipulation"/>
    <meta name="twitter:creator" content="@Tianxing Chen"/>


    <!-- extra metadata — unknown support -->
    <meta property="og:type" content="article"/>
    <meta property="article:section" content="Research"/>
    <meta property="article:tag" content="Robotics"/>
    <meta property="article:tag" content="Machine Learning"/>

</head>
<body>

<div class="title_left">
    <h1>
        <span style="color:#65a487; font-size: larger;">Robo</span><span style="color:#68349a; font-size: larger;">Twin</span>
        <span>2.0: A Scalable Data Generator and Benchmark with <br>Strong Domain Randomization for Robust Bimanual Robotic Manipulation</span>
    </h1>      
    <div class="author-container">
        <div class="author-name">
            <strong><a href="https://tianxingchen.github.io/" target="_blank">Tianxing Chen</a></strong><sup>2,15*</sup>,
        </div>
        <div class="author-name">
            <strong>Zanxin Chen</strong><sup>3,5*</sup>,
        </div>
        <div class="author-name">
            <strong>Baijun Chen</strong><sup>7*</sup>,
        </div>
        <div class="author-name">
            <strong>Zijian Cai</strong><sup>3,5*</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://10-oasis-01.github.io" target="_blank">Yibin Liu</a></strong><sup>13*</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong><a href="https://kolakivy.github.io/" target="_blank">Qiwei Liang</a></strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Zixuan Li</strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Xianliang Lin</strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://geyiheng.github.io" target="_blank">Yiheng Ge</a></strong><sup>1</sup>,
        </div>
        <div class="author-name">
            <strong>Zhenyu Gu</strong><sup>8</sup>,
        </div>
        <div class="author-name">
            <strong>Weiliang Deng</strong><sup>3,11</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong>Yubin Guo</strong><sup>9</sup>,
        </div>
        <div class="author-name">
            <strong>Tian Nian</strong><sup>3,5</sup>,
        </div>
        <div class="author-name">
            <strong>Xuanbing Xie</strong><sup>12</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://www.linkedin.com/in/yusen-qin-5b23345b/" target="_blank">Qiangyu Chen</a></strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Kailun Su</strong><sup>5</sup>,
        </div>
        <div class="author-name">
            <strong>Tianling Xu</strong><sup>10</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong><a href="http://luoping.me/" target="_blank">Guodong Liu</a></strong><sup>6</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://aaron617.github.io/" target="_blank">Mengkang Hu</a></strong><sup>2</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://c7w.tech/about" target="_blank">Huan-ang Gao</a></strong><sup>6,15</sup>,
        </div>
        <div class="author-name">
            <strong>Kaixuan Wang</strong><sup>2,15</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://liang-zx.github.io/" target="_blank">Zhixuan Liang</a></strong><sup>2,3</sup>,
        </div>
        <div class="flex-break"></div> 
        <div class="author-name">
            <strong><a href="https://www.linkedin.com/in/yusen-qin-5b23345b/" target="_blank">Yusen Qin</a></strong><sup>4,6</sup>,
        </div>
        <div class="author-name">
            <strong>Xiaokang Yang</strong><sup>1</sup>,
        </div>
        <div class="author-name">
            <strong><a href="http://luoping.me/" target="_blank">Ping Luo</a></strong><sup>2,14✉</sup>,
        </div>
        <div class="author-name">
            <strong><a href="https://yaomarkmu.github.io/" target="_blank">Yao Mu</a></strong><sup>1,3✉</sup>
        </div>
    </div>
    
    <style>
        .author-container {
            color: black;
            font-family: "Source Serif 4", serif;
            font-weight: normal;
            font-size: 1vw;
            padding-top: 1.0vw;
            justify-content: center; 
            display: flex;
            flex-wrap: wrap; 
            align-items: center; 
            text-align: center; 
            white-space: normal; 
        }
        .author-name {
            margin: 0 0.0em;
            white-space: normal; 
        }
        .flex-break {
            width: 100%; 
        }
    </style>
        
    <div class="affiliation" style="white-space: nowrap;">
        <p><sup>1</sup>SJTU ScaleLab<sup>†</sup>,&nbsp;<sup>2</sup>HKU MMLab<sup>†</sup>,&nbsp; 
           <sup>3</sup>Shanghai AI Lab,&nbsp; <sup>4</sup>D-Robotics,&nbsp; 
           <sup>5</sup>SZU,&nbsp; <sup>6</sup>THU,&nbsp;
           <sup>7</sup>NJU,&nbsp; <br> <sup>8</sup>FDU,&nbsp;
           <sup>9</sup>USTC,&nbsp; <sup>10</sup>SUSTech,&nbsp;
           <sup>11</sup>SYSU,&nbsp; <sup>12</sup>CSU,&nbsp;
           <sup>13</sup>NEU,&nbsp; <sup>14</sup>HKU-Shanghai ICRC,&nbsp;
           <sup>15</sup>Lumina EAI,&nbsp; <br> <sup>*</sup>Equal contribution&nbsp;
           <sup>✉</sup>Corresponding authors&nbsp;<sup>†</sup>Equally leading organizations&nbsp;
        </p>
    </div>
    <br>
    <img src="assets/images/institute-logo.png" alt="描述信息" style="display: block; margin: auto; width: 50%;">
    <br><br>
    <div class="button-container">
         <a href="" target="_blank" class="button_large"><i class="ai ai-arxiv"></i>&emsp14;arXiv (coming soon)</a>
        <a href="http://robotwin-platform.github.io/paper.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
        <a href="https://github.com/robotwin-Platform/RoboTwin" target="_blank" class="button_large"><i class="fa fa-github"></i>&emsp14;Code</a>
        <a href="https://robotwin-platform.github.io/doc/" target="_blank" class="button_large"><i class="fa-regular fa-file-lines"></i>&emsp14;Document</a>
        <!-- <a href="https://x.com/ZeYanjie/status/1846024050067538399" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;TL;DR</a> -->
        <!-- <a href="https://youtu.be/6H2MkMetmFk" target="_blank" class="button"><i class="fa-brands fa-youtube"></i>&emsp14;YouTube</a> -->
    </div>
</div>
           
    
<div id="abstract">
    <h1>Abstract</h1>
    <p>
        Simulation-based data synthesis has emerged as a powerful paradigm for enhancing real-world robotic manipulation. However, existing synthetic datasets remain insufficient for robust bimanual manipulation due to two challenges: (1) the lack of an efficient, scalable data generation method for novel tasks, and (2) oversimplified simulation environments that fail to capture real-world complexity. We present RoboTwin 2.0, a scalable simulation framework that enables automated, large-scale generation of diverse and realistic data, along with unified evaluation protocols for dual-arm manipulation. We first construct RoboTwin-OD, a large-scale object library comprising 731 instances across 147 categories, each annotated with semantic and manipulation-relevant labels. Building on this foundation, we develop an expert data synthesis pipeline that combines multimodal large language models (MLLMs) with simulation-in-the-loop refinement to generate task-level execution code automatically. To improve sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization along five axes: clutter, lighting, background, tabletop height and language instructions, thereby enhancing data diversity and policy robustness. We instantiate this framework across 50 dual-arm tasks spanning five robot embodiments, and pre-collect over 100,000 domain-randomized expert trajectories. Empirical evaluation shows a 10.9% gain in code generation success rate and improved generalization to novel real-world conditions. A vision-language-action (VLA) model fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%) on unseen scene real-world tasks, while zero-shot models trained exclusively on our synthetic data attain a 228% relative gain, demonstrating strong generalization without real-world supervision. We release the data generator, benchmark, pre-collected dataset, and code to support scalable research in robust bimanual manipulation.
    </p>
    <br>
    <!-- <div class="publication-video">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/6H2MkMetmFk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
    </div> -->
    <div style="display: flex; justify-content: center; margin-top: 20px;">
        <video 
          src="https://robotwin-platform.github.io/RoboTwin2.0_video.mp4" 
          controls 
          autoplay 
          muted 
          loop 
          style="width: 100%; height: auto;"
        ></video>
      </div>
</div>

<hr class="rounded">



<div id="overview">
    <h1>Previous Works</h1>
    <p>
        <b>[CVPR 2025 Highlight]</b> <a href="https://arxiv.org/abs/2504.13059">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</a><br>
        <b>[CVPR 2025 Challenge@MEIS Workshop]</b> The Technical report is coming soon !<br>
        <b>[ECCV 2024 MAAS Workshop Best Paper]</b> <a href="https://arxiv.org/abs/2409.02920">RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</a><br>
        <b>[第十九届挑战杯官方赛题]</b> <a href="https://2025.tiaozhanbei.net/media/ckeditor_uploads/49/2025/05/14/4.%E3%80%90%E9%A2%98%E7%9B%AE%E5%9B%9B%E3%80%91%E7%AB%AF%E4%BE%A7%E5%8F%AF%E9%83%A8%E7%BD%B2%E7%9A%84%E5%8F%8C%E8%87%82%E6%93%8D%E4%BD%9C%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1.pdf">赛题链接</a>
    </p>
    <h1>Overview of RoboTwin 2.0</h1>
    <img src="assets/images/teaser.png" alt="描述信息" style="display: block; margin: auto; width: 70%;">
    <p>
        RoboTwin 2.0 is a scalable framework for data generation and benchmarking in bimanual robotic manipulation. It integrates an expert data generation pipeline and a 50-task benchmark built on the RoboTwin Object Dataset (731 objects, 147 categories). A multimodal language model agent enables automatic task program synthesis, while flexible dual-arm configurations facilitate scalable and diverse data collection. Policies trained on RoboTwin 2.0 data demonstrate improved robustness and generalization to unseen environments.
    </p>

    <br>
    <hr class="rounded">


    <h1>RoboTwin Object Dataset</h1>
    <img src="assets/images/robotwin-od.png" alt="RoboTwin-OD" style="display: block; margin: auto; width: 70%;">

    <p>
        To enhance both manipulation capability and visual understanding, we construct a large-scale object dataset with rich semantic annotations, called <b>RoboTwin-OD</b>, covering 147 categories and 731 diverse objects. Specifically, this includes 534 instances across 111 categories with custom-generated and optimized meshes, 153 objects from 27 categories in Objaverse, and 44 articulated object instances from 9 categories in SAPIEN PartNet-Mobility. Objects from all sources, including Objaverse, are used for cluttered scene construction, with Objaverse specifically serving to further increase the visual and semantic diversity of distractor objects. Additionally, we develop a comprehensive surface and background texture library using generative AI and human-in-the-loop verification to ensure both diversity and realism. The dataset is available at <a href="https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/objects">https://huggingface.co/datasets/TianxingChen/RoboTwin2.0/tree/main/objects</a>.
    </p>

    <hr class="rounded">


    <h1>50 RoboTwin 2.0 Bimanual Tasks</h1>
    <img src="assets/images/50-tasks.gif" alt="描述信息" style="display: block; margin: auto; width: 70%;">

    <p>
        Building on our automated task generation framework, embodiment-adaptive behavior synthesis, and the large-scale object asset library RoboTwin-OD, we construct a suite of over 50 dual-arm collaborative manipulation tasks. In addition, we support data collection and evaluation across 5 distinct robot platforms, enabling comprehensive benchmarking of manipulation policies. The complete task set is available at <a href="http://robotwin-platform.github.io/doc/tasks/">http://robotwin-platform.github.io/doc/tasks/</a>.
    </p>

    <br>
    <hr class="rounded">
    
    <h1>Multi-Embodiments Support</h1>
    <img src="assets/images/cross-embodiment.png" alt="描述信息" style="display: block; margin: auto; width: 40%;">


    <br>
    <hr class="rounded">
    
    <h1>Domain Randomization</h1>
    <img src="assets/images/randomization.png" alt="RoboTwin-OD" style="display: block; margin: auto; width: 70%;">

    <p>
        To improve policy robustness to real-world environmental variability, we apply domain randomization across five key dimensions: (1) cluttered placement of task-irrelevant objects, (2) background textures, (3) lighting conditions, (4) tabletop heights, and (5) diverse language instructions. This systematic diversification enriches the training data distribution and significantly improves generalization to unseen scenarios.
    </p>
    
    <br>
    <hr class="rounded">

    <h1>Conclusions</h1>

    <p>
        This paper presented RoboTwin 2.0, a scalable simulation framework for generating diverse, high-fidelity expert data to support robust bimanual manipulation. Our system integrates MLLM-based task generation, embodiment-adaptive behavior synthesis, and comprehensive domain randomization to address key limitations in prior synthetic datasets.
        <br>
By leveraging an annotated object library and automating trajectory generation, RoboTwin 2.0 produces data with rich visual, linguistic, and physical diversity while minimizing manual engineering effort. Experiments demonstrate its effectiveness in improving policy robustness to cluttered environments, generalization to unseen tasks, and cross-embodiment manipulation.
<br>
These findings highlight the importance of scalable, automated generation of semantically rich, domain-randomized data for learning robust manipulation policies. RoboTwin 2.0 provides a foundation for unified benchmarks and scalable sim-to-real pipelines, with future work focusing on real-world deployment and multi-object task complexity.
    </p>


    <h1>BibTeX</h1>
    <p>Coming Soon !</p>
    <!-- <p class="bibtex">@article{ze2024humanoid_manipulation,<br>
        &nbsp;&nbsp;title &nbsp;&nbsp;= {Generalizable Humanoid Manipulation with 3D Diffusion Policies},<br>
        &nbsp;&nbsp;author &nbsp;= {Yanjie Ze and Zixuan Chen and Wenhao Wang and Tianyi Chen and Xialin He and Ying Yuan and Xue Bin Peng and Jiajun Wu},<br>
        &nbsp;&nbsp;year &nbsp;&nbsp;&nbsp;= {2024},<br>
        &nbsp;&nbsp;journal = {arXiv preprint arXiv:2410.10803}<br>
        }
    </p> -->
    <br>
</div>
</body>

</html>
